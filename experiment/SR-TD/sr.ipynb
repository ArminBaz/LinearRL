{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import gym_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abizzle/anaconda3/envs/gym/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'agent': array([0, 0]), 'target': array([14, 14])}, {'distance': 28.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"simple-15x15\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment specifics\n",
    "actions = np.arange(env.action_space.n, dtype=int)\n",
    "random_action = env.unwrapped.random_action()\n",
    "start_loc = env.unwrapped.start_loc\n",
    "target_loc = env.unwrapped.target_loc\n",
    "maze = env.unwrapped.maze\n",
    "size = maze.size\n",
    "target_locs = [target_loc]\n",
    "maze_len = maze.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' '0' '1' '0' '0' '0' '1' '0' '0' '1' '0' '0' '0' '0' '0']\n",
      " ['0' '0' '0' '1' '0' '0' '0' '1' '0' '0' '1' '0' '0' '0' '0']\n",
      " ['0' '0' '0' '1' '0' '0' '0' '1' '0' '0' '1' '0' '0' '0' '0']\n",
      " ['0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '1' '1' '0' '0' '0']\n",
      " ['0' '0' '1' '0' '0' '0' '0' '1' '1' '0' '0' '1' '0' '0' '0']\n",
      " ['0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '0' '0' '1' '1' '1']\n",
      " ['0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '1' '1' '0' '0']\n",
      " ['0' '0' '0' '1' '1' '1' '1' '0' '0' '0' '0' '1' '0' '0' '0']\n",
      " ['0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0' '0']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0']\n",
      " ['0' '0' '0' '0' '1' '1' '1' '0' '1' '1' '1' '1' '0' '0' '0']\n",
      " ['0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0']\n",
      " ['0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '1' '0' '0' '1' '1']\n",
      " ['0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' 'G']]\n"
     ]
    }
   ],
   "source": [
    "print(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrices needed for calculation\n",
    "DR = np.zeros((size,size))\n",
    "V = np.zeros((size))\n",
    "one_hot = np.eye(size)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.95\n",
    "significant_improvement = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_col_to_index(row, col, len):\n",
    "    \"\"\"\n",
    "    Converts (row,col) to an index in array\n",
    "    \"\"\"\n",
    "    return row*len + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR_no_action():\n",
    "\n",
    "\t''' This class defines a reinforcement learning agent that \n",
    "\tlearns the state-state successor representation without taking actions. \n",
    "\tThus, the resulting SR matrix is in the service of prediction. \n",
    "\n",
    "\tInitalization parameters\n",
    "\n",
    "\tgamma: discount param\n",
    "\talpha: learning rate\n",
    "\tp_sample: probability of sampling different options, only relevant for testing poilcy dependence\n",
    "\tNUM_STATES: the number of states in the environment to intialize matrices\n",
    "\n",
    "\tIda Momennejad, 2019'''\n",
    "\n",
    "\tdef __init__(self, gamma, alpha, p_sample, NUM_STATES):\n",
    "\t\tself.gamma = gamma # discount factor\n",
    "\t\tself.alpha = alpha # learning rate\n",
    "\t\tself.p_sample = p_sample # p(sampling options)\n",
    "\n",
    "\t\t# Initalize M with I instead of zeors\n",
    "\t\t#self.M= np.zeros([NUM_STATES, NUM_STATES]) # M: state-state SR    \t\n",
    "\t\tself.M= np.eye(NUM_STATES) # M: state-state SR    \t\n",
    "\n",
    "\t\tself.W= np.zeros(NUM_STATES) # W: value weights, 1D\n",
    "\t\tself.onehot=np.eye(NUM_STATES) # onehot matrix, for updating M\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself.V= np.zeros(NUM_STATES) # value function\n",
    "\t\tself.biggest_change = 0\n",
    "\t\tself.significant_improvement = 0.001 # convergence threshold\n",
    "\t\t# policy: not revelant in exp 1, agent is passively moved\n",
    "\t\t#\t\t  but in Exp2 we keep updating it to get the optimal policy\n",
    "    \t# self.Pi = np.zeros([NUM_STATES], dtype=int)  \n",
    "\t\tself.epsilon = .1\n",
    "\t\tself.memory=[]\n",
    "\n",
    "\tdef step(self, s, s_new, reward):\n",
    "\n",
    "\t\told_v = self.get_value()\n",
    "\n",
    "\t\tself.update_memory(s, s_new)\n",
    "\t\tself.update_SR(s, s_new)\n",
    "\t\tself.update_W(s, s_new, reward)\n",
    "\n",
    "\t\tself.update_biggest_change(old_v[s], s)\n",
    "\n",
    "        ########## update policy  ##############\n",
    "        #Pi[s] = action\n",
    "        # M, W = dyna_replay(memory, M, W, episodes)\n",
    "\n",
    "\tdef onehot_row(self, successor_s):\t\n",
    "\t\trow = np.zeros( len(self.W)) \n",
    "\t\trow[successor_s] = 1\n",
    "\t\treturn row\n",
    "\n",
    "\tdef update_SR(self, s, s_new):\n",
    "\n",
    "\t\tonehot_row = self.onehot_row(s_new)\n",
    "\t\tSR_TD_error = onehot_row + self.gamma * self.M[s_new] -self.M[s]  \n",
    "\n",
    "\n",
    "\t\t# learning by element, as opposed to by row\n",
    "\t\tself.M[s, s_new] =  self.M[s,s_new] + .2*SR_TD_error[s_new]\n",
    "\n",
    "\t\t# self.M[s] = (1-self.alpha)* self.M[s] + self.alpha * ( self.onehot[s] + self.gamma * self.M[s_new]  )\n",
    "\t\t\n",
    "\n",
    "\tdef update_W(self, s, s_new, reward):\n",
    "\n",
    "\t\t''' Update value weight vector. \n",
    "\t\tIt computes the normalized feature vector * reward PE.\n",
    "\t\tHere reward function would be sufficient. The same, \n",
    "\t\tbut R is easier. We use W in plos comp biol 2017 paper, to \n",
    "\t\taccount for BG weights allowing dopamine similarities \n",
    "\t\tbetween  MF and MB learning.'''\n",
    "\n",
    "\t\t# future notes: 27 feb 2019: in paper both get updated with every transition\n",
    "\t\t# better to do batch updates. W updated every step, but M \n",
    "\t\t# updated every couple of steps with dyna\n",
    "\t\t# like feature learning.\n",
    "\t\t# all rules are correct, but in practice for TD learning on features\n",
    "\t\t# a little weird to learn feature vector with every step\n",
    "\t\t# normally features are stable over the task.\n",
    "\n",
    "\t\tnorm_feature_rep = self.M[s] / ( self.M[s]@self.M[s].T ) \n",
    "\n",
    "\t\t# Compute the values of s and s_prime, then the prediction error\n",
    "\n",
    "\t\tV_snew = self.M[s_new]@self.W  \n",
    "\t\tV_s    = self.M[s]@self.W \t\t                          \n",
    "\t\tw_pe = ( reward + self.gamma*V_snew - V_s ).squeeze()        \n",
    "\n",
    "\t\t# Update W with the same learning rate\n",
    "\t\t# future: this could be different\n",
    "\t\tself.W += self.alpha * w_pe *norm_feature_rep\n",
    "\n",
    "\tdef get_value(self):\n",
    "\t\t''' Combine the successor representation M & value weight W\n",
    "\t\t\tto determine the value of different options'''\n",
    "\n",
    "\t\tself.V = self.M@self.W\t\t\n",
    "\t\treturn self.V\n",
    "\n",
    "\tdef update_memory(self, s, s_new):\n",
    "\t\t''' Save current state and the state it visited in one-step\n",
    "\t\t\tto memory. This is used in the Dyna version for replay.'''\n",
    "\n",
    "\t\tself.memory.append([s, s_new])\n",
    "\n",
    "\t\n",
    "\tdef update_biggest_change(self, old_v_m, s):\n",
    "\t\t''' Coompute the change in value, see if it is higher\n",
    "\t\t\tthan the present max change, if so, update biggest_change '''\n",
    "\n",
    "\t\tV=self.get_value()\n",
    "\t\tself.biggest_change = max(self.biggest_change, np.abs(old_v_m - V[s]))   \n",
    "\t\tself.check_converegnce()         \n",
    "\t\n",
    "\tdef check_converegnce(self):\n",
    "\t\t''' If statement is true, convergence has reached. '''\n",
    "\n",
    "\t\tself.convergence= self.biggest_change < self.significant_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SRclass_nathum_exp1(env, gamma, alpha, p_sample=None, verbose=0):\n",
    "\n",
    "    ''' This function uses the reinfrocement learning agent class in \n",
    "        SR_no_action.py to learn.\n",
    "        Here the function takes the environment from Experiment 1 in our\n",
    "        Nat Hum Beh paper & learns predictive representations with the \n",
    "        specified learning rate and scale. \n",
    "\n",
    "        Note: This is not SR dyna, nor SR-MB. \n",
    "        This agent only learns the SR.\n",
    "\n",
    "        Outputs:\n",
    "\n",
    "        M: SR matrix \n",
    "        W: value weights W\n",
    "        memory: memory of episodes\n",
    "        episodies: # episodes it takes to reach convergence\n",
    "\n",
    "        Ida Momennejad, NYC, 2019'''\n",
    "\n",
    "    if p_sample==None:\n",
    "        p_sample= [.5,.5]\n",
    "\n",
    "    SR_agent = SR_no_action(gamma, alpha, p_sample, env.unwrapped.maze.size)\n",
    "    episodes = 0\n",
    "    done = False\n",
    "   \n",
    "    while True:\n",
    "        SR_agent.biggest_change = 0\n",
    "        s = env.unwrapped.start_loc\n",
    "        s = row_col_to_index(s[0],s[1],maze_len)\n",
    "\n",
    "        done = False\n",
    "        while not done: # go through trajectory till the end\n",
    "            # Get a random action\n",
    "            a = env.unwrapped.random_action()\n",
    "            obs, reward, done, _, _ = env.step(a)\n",
    "            s_new = row_col_to_index(obs[\"agent\"][0], obs[\"agent\"][1], maze_len)\n",
    "            SR_agent.step(s, s_new, reward)\n",
    "            \n",
    "            s = s_new\n",
    "        \n",
    "        if verbose==2:\n",
    "            if episodes % verbose ==0:                    \n",
    "                print(f'SR training episode #{episodes} Done.')\n",
    "        episodes += 1\n",
    "\n",
    "        if SR_agent.convergence:\n",
    "            if verbose==2:\n",
    "                print (episodes,' training episodes/iterations done')\n",
    "            break\n",
    "\n",
    "    return SR_agent.M, SR_agent.W , SR_agent.memory, episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 2) create SR agent, let it learn the environment\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m M, W, mem, total_episodes \u001b[39m=\u001b[39m SRclass_nathum_exp1(env, gamma, alpha)\n",
      "\u001b[1;32m/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     obs, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     s_new \u001b[39m=\u001b[39m row_col_to_index(obs[\u001b[39m\"\u001b[39m\u001b[39magent\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m], obs[\u001b[39m\"\u001b[39m\u001b[39magent\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m], maze_len)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     SR_agent\u001b[39m.\u001b[39;49mstep(s, s_new, reward)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     s \u001b[39m=\u001b[39m s_new\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m verbose\u001b[39m==\u001b[39m\u001b[39m2\u001b[39m:\n",
      "\u001b[1;32m/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, s, s_new, reward):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \told_v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_value()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_memory(s, s_new)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abizzle/Research/LinearRL/experiment/SR-TD/sr.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_SR(s, s_new)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2) create SR agent, let it learn the environment\n",
    "M, W, mem, total_episodes = SRclass_nathum_exp1(env, gamma, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it out\n",
    "\n",
    "# Calculate the value matrix\n",
    "V = M@W\n",
    "\n",
    "v_maze = np.zeros_like(maze)\n",
    "for row in range(v_maze.shape[0]):\n",
    "    for col in range(v_maze.shape[1]):\n",
    "        if maze[row, col] == \"1\":\n",
    "            v_maze[row,col] = \"BAR\"\n",
    "            continue\n",
    "        idx = row_col_to_index(row,col,maze_len)\n",
    "        v_maze[row,col] = round(V[idx], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2.2158782', '0.232469', 'BAR', '0.00063', '-0.0003467',\n",
       "        '3.21e-05', 'BAR', '-0.0014005', '0.0013277', 'BAR', '0.0',\n",
       "        '0.0', '0.0', '0.0', '0.0'],\n",
       "       ['0.2372448', '0.2847028', '0.0020548', 'BAR', '-0.0001539',\n",
       "        '0.00017', '-8.03e-05', 'BAR', '0.0001798', '-0.0003039', 'BAR',\n",
       "        '0.0', '0.0', '0.0', '0.0'],\n",
       "       ['0.132729', '-0.0006006', '-0.0007308', 'BAR', '0.000126',\n",
       "        '7.41e-05', '-4.2e-05', 'BAR', '-1.77e-05', '0.0005734', 'BAR',\n",
       "        '0.0', '0.0', '0.0', '0.0'],\n",
       "       ['0.0008737', '0.0049573', 'BAR', '0.0001837', '6.73e-05',\n",
       "        '-0.0003644', '0.0005158', 'BAR', '0.0002517', '0.0009825',\n",
       "        'BAR', 'BAR', '0.0', '0.0', '0.0'],\n",
       "       ['-0.0018272', '-0.0088824', 'BAR', '-0.0001209', '9.15e-05',\n",
       "        '2.1e-06', '0.0001697', 'BAR', 'BAR', '-0.0007858', '0.0002091',\n",
       "        'BAR', '0.0', '0.0', '0.0'],\n",
       "       ['0.0020045', '0.0025835', 'BAR', '9.35e-05', '0.0002054',\n",
       "        '0.0003045', '-0.0005983', 'BAR', '-0.0003616', '0.000428',\n",
       "        '0.0003945', '-0.0005756', 'BAR', 'BAR', 'BAR'],\n",
       "       ['-0.0040863', '-0.0046518', '0.0028801', 'BAR', '-0.000121',\n",
       "        '4.87e-05', '-6.3e-06', '0.0004898', '-0.0001666', '-0.002187',\n",
       "        '0.0005137', 'BAR', 'BAR', '-0.0057955', '0.0032416'],\n",
       "       ['-0.0032514', '0.0138381', '0.0004758', 'BAR', 'BAR', 'BAR',\n",
       "        'BAR', '-2.95e-05', '-0.0012458', '0.0002532', '0.0001928',\n",
       "        'BAR', '-0.0005827', '-0.0062637', '0.0305002'],\n",
       "       ['0.0162928', '-0.0039975', '-0.0153842', 'BAR', '-0.0032731',\n",
       "        '0.0012279', '0.0006018', '-0.0005344', 'BAR', '0.0052934',\n",
       "        '-0.0054882', 'BAR', '-0.0006404', '0.0083982', '0.0008711'],\n",
       "       ['-0.0027759', '-0.0128011', '0.0003744', '0.0024789',\n",
       "        '-0.0011367', '-0.0037621', '0.0006662', 'BAR', 'BAR',\n",
       "        '-0.0057945', '-0.0046903', '-0.0001365', '-0.0198958',\n",
       "        '0.0022245', '-0.0268354'],\n",
       "       ['-0.0009072', '0.0036688', '-0.0069001', '-0.0051349', 'BAR',\n",
       "        'BAR', 'BAR', '0.0331323', 'BAR', 'BAR', 'BAR', 'BAR',\n",
       "        '0.017336', '-0.0032402', '0.0181369'],\n",
       "       ['-0.0004278', '0.0010564', '0.0046368', '-0.0034155', 'BAR',\n",
       "        '0.0046229', '0.0013506', '-0.0038414', '-0.013534', '0.0207886',\n",
       "        'BAR', '-0.1852988', '0.1129825', '0.0140958', '-0.0027603'],\n",
       "       ['0.0006622', '-0.0035963', '-0.006185', '0.0035735', 'BAR',\n",
       "        '-0.0060146', '-0.0032241', '0.0030123', '0.0145705',\n",
       "        '0.0203065', 'BAR', '0.0283833', '-0.4362142', 'BAR', 'BAR'],\n",
       "       ['0.0114095', '0.0005353', '-0.0081097', '-0.0080885', 'BAR',\n",
       "        '0.0054727', '0.0125594', '0.0037285', '0.0085632', '-0.0254388',\n",
       "        '0.0581826', '-0.3019812', '4.6365317', '4.7444538',\n",
       "        '11.0769953'],\n",
       "       ['-0.0058463', '-0.0006443', '0.004586', '-0.0071209', 'BAR',\n",
       "        '0.0178696', '0.0045849', '-0.0095549', '0.0122556', '0.0273879',\n",
       "        '0.127286', '2.026289', '2.4023438', '10.7667022', '5.3864799']],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
